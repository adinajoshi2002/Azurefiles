{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df0c013-5c75-469d-b3d8-2b924f35216f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##RDD functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f04718-99d6-44e1-b30e-82abec3994db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method textFile in module pyspark.context:\n\ntextFile(name: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[str] method of dbruntime.spark_connection.RemoteContext instance\n    Read a text file from HDFS, a local file system (available on all\n    nodes), or any Hadoop-supported file system URI, and return it as an\n    RDD of Strings. The text files must be encoded as UTF-8.\n    \n    .. versionadded:: 0.7.0\n    \n    Parameters\n    ----------\n    name : str\n        directory to the input data files, the path can be comma separated\n        paths as a list of inputs\n    minPartitions : int, optional\n        suggested minimum number of partitions for the resulting RDD\n    use_unicode : bool, default True\n        If `use_unicode` is False, the strings will be kept as `str` (encoding\n        as `utf-8`), which is faster and smaller than unicode.\n    \n        .. versionadded:: 1.2.0\n    \n    Returns\n    -------\n    :class:`RDD`\n        RDD representing text data from the file(s).\n    \n    See Also\n    --------\n    :meth:`RDD.saveAsTextFile`\n    :meth:`SparkContext.wholeTextFiles`\n    \n    Examples\n    --------\n    >>> import os\n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     path1 = os.path.join(d, \"text1\")\n    ...     path2 = os.path.join(d, \"text2\")\n    ...\n    ...     # Write a temporary text file\n    ...     sc.parallelize([\"x\", \"y\", \"z\"]).saveAsTextFile(path1)\n    ...\n    ...     # Write another temporary text file\n    ...     sc.parallelize([\"aa\", \"bb\", \"cc\"]).saveAsTextFile(path2)\n    ...\n    ...     # Load text file\n    ...     collected1 = sorted(sc.textFile(path1, 3).collect())\n    ...     collected2 = sorted(sc.textFile(path2, 4).collect())\n    ...\n    ...     # Load two text files together\n    ...     collected3 = sorted(sc.textFile('{},{}'.format(path1, path2), 5).collect())\n    \n    >>> collected1\n    ['x', 'y', 'z']\n    >>> collected2\n    ['aa', 'bb', 'cc']\n    >>> collected3\n    ['aa', 'bb', 'cc', 'x', 'y', 'z']\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.sparkContext.textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac87cc81-16bc-43d7-8267-2b33b533472d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,1,957,1,299.98,299.98', '2,2,1073,1,199.99,199.99', '3,2,502,5,250.0,50.0', '4,2,403,1,129.99,129.99', '5,4,897,2,49.98,24.99']\n"
     ]
    }
   ],
   "source": [
    "#we can read data from file in 2 ways \n",
    "order_items=sc.textFile('dbfs:/FileStore/order_items')\n",
    "print(list(order_items.take(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dab0aa09-5bab-4071-9797-d9d5f38d1988",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,957,1,299.98,299.98\n2,2,1073,1,199.99,199.99\n3,2,502,5,250.0,50.0\n4,2,403,1,129.99,129.99\n5,4,897,2,49.98,24.99\n"
     ]
    }
   ],
   "source": [
    "rdd=spark.sparkContext.textFile('dbfs:/FileStore/order_items')\n",
    "for i in rdd.take(5): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d122fa6c-9fe7-433b-b555-734a80fb8575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of partition\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76fb9107-57a7-40b8-bed3-083401e58162",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[87942, 84256]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gives no of records in each partition\n",
    "rdd.glom().map(len).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6019efbb-b14b-48af-86e8-bdca3d038475",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parallelize in module pyspark.context:\n\nparallelize(c: Iterable[~T], numSlices: Optional[int] = None) -> pyspark.rdd.RDD[~T] method of dbruntime.spark_connection.RemoteContext instance\n    Distribute a local Python collection to form an RDD. Using range\n    is recommended if the input represents a range for performance.\n    \n    .. versionadded:: 0.7.0\n    \n    Parameters\n    ----------\n    c : :class:`collections.abc.Iterable`\n        iterable collection to distribute\n    numSlices : int, optional\n        the number of partitions of the new RDD\n    \n    Returns\n    -------\n    :class:`RDD`\n        RDD representing distributed collection.\n    \n    Examples\n    --------\n    >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n    [[0], [2], [3], [4], [6]]\n    >>> sc.parallelize(range(0, 6, 2), 5).glom().collect()\n    [[], [0], [], [2], [4]]\n    \n    Deal with a list of strings.\n    \n    >>> strings = [\"a\", \"b\", \"c\"]\n    >>> sc.parallelize(strings, 2).glom().collect()\n    [['a'], ['b', 'c']]\n\n"
     ]
    }
   ],
   "source": [
    "#create rdd from python list\n",
    "lst=[1,2,3,4,5]\n",
    "help(spark.sparkContext.parallelize)#takes list as input and we can give nof of partition also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da117c51-6b38-41bb-a178-5fedf67d74cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyt_rdd=spark.sparkContext.parallelize(lst)\n",
    "print(list(pyt_rdd.take(5)))#action\n",
    "pyt_rdd.collect()#action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0df0adc-ecf1-4c13-9979-a27c55ca5893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f3f712-b57f-4f16-80c1-7948bbe44b7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2013-07-25 00:00:00.0,11599,CLOSED\n2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\n3,2013-07-25 00:00:00.0,12111,COMPLETE\n4,2013-07-25 00:00:00.0,8827,CLOSED\n5,2013-07-25 00:00:00.0,11318,COMPLETE\n"
     ]
    }
   ],
   "source": [
    "ord=spark.sparkContext.textFile('dbfs:/FileStore/order')\n",
    "for i in ord.take(5): print(i)                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4252ab46-a36c-4ddd-be64-e74a8dd5f037",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11599\n256\n12111\n8827\n11318\n"
     ]
    }
   ],
   "source": [
    "# project all the order_id\n",
    "ord_id=ord.map(lambda x: x.split(',')[2])\n",
    "for i in ord_id.take(5):print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027f348d-2c0a-49d9-acca-98e8321795a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('11599', 'CLOSED')\n('256', 'PENDING_PAYMENT')\n('12111', 'COMPLETE')\n('8827', 'CLOSED')\n('11318', 'COMPLETE')\n"
     ]
    }
   ],
   "source": [
    "# project all orders and status\n",
    "ord_id_status=ord.map(lambda x: (x.split(',')[2],x.split(',')[3]))\n",
    "for i in ord_id_status.take(5): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434308cc-01fe-450a-b323-23fe436b1496",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11599&CLOSED\n256&PENDING_PAYMENT\n12111&COMPLETE\n8827&CLOSED\n11318&COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# combine order id and status with '&'\n",
    "ord_id_statu=ord.map(lambda x: x.split(',')[2] + '&' +x.split(',')[3])\n",
    "for i in ord_id_statu.take(5): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c8b7b3-4689-4268-a3c5-13ec9a392e20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013/07/25\n2013/07/25\n2013/07/25\n2013/07/25\n2013/07/25\n"
     ]
    }
   ],
   "source": [
    "#convert the order date into yyy/mm/dd formate\n",
    "order_date=ord.map(lambda x: x.split(',')[1].split(' ')[0].split('-')[0]\n",
    "    +'/'+ x.split(',')[1].split(' ')[0].split('-')[1]\n",
    "    +'/'+ x.split(',')[1].split(' ')[0].split('-')[2])\n",
    "for i in order_date.take(5): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae7c936-5d38-44e2-b6ab-b09189b76f38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('11599', '1,2013-07-25 00:00:00.0,11599,CLOSED')\n('256', '2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT')\n('12111', '3,2013-07-25 00:00:00.0,12111,COMPLETE')\n('8827', '4,2013-07-25 00:00:00.0,8827,CLOSED')\n('11318', '5,2013-07-25 00:00:00.0,11318,COMPLETE')\n"
     ]
    }
   ],
   "source": [
    "#create key-value pair key=orde_id,value=whole record\n",
    "ord_rdd=ord.map(lambda x: (x.split(',')[2],x))\n",
    "for i in ord_rdd.take(5):print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbe0be8-8138-4e51-bbcb-826bb119178e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e696c0c-8994-432d-a9ae-426e3dff9aa9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n2013-07-25 00:00:00.0\n11599\nCLOSED\n2\n"
     ]
    }
   ],
   "source": [
    "#flatMap will make each value within a map records as each record\n",
    "fm_ord=ord.flatMap(lambda x: x.split(','))\n",
    "for i in fm_ord.take(5): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868d36be-baed-4919-ba49-72f8b876c38f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 2)\n('CLOSED', 7556)\n('256', 11)\n('12111', 7)\n('4', 7)\n"
     ]
    }
   ],
   "source": [
    "#count the words\n",
    "word_count=ord.flatMap(lambda x: x.split(',')).map(lambda w: (w,1)).reduceByKey(lambda x,y:x+y)\n",
    "for i in word_count.take(5): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c86cd84b-be02-4735-94be-5e3f03f20391",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7863cef2-e7be-421e-9494-5e0efa4207f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('COMPLETE', '25882,2014-01-01 00:00:00.0,4598,COMPLETE')\n('COMPLETE', '25888,2014-01-01 00:00:00.0,6735,COMPLETE')\n('COMPLETE', '25889,2014-01-01 00:00:00.0,10045,COMPLETE')\n('CLOSED', '25891,2014-01-01 00:00:00.0,3037,CLOSED')\n('COMPLETE', '25895,2014-01-01 00:00:00.0,1044,COMPLETE')\n"
     ]
    }
   ],
   "source": [
    "filter_ord=ord.map(lambda x: (x.split(',')[3],x)).filter(lambda x:x[0] in ['CLOSED','COMPLETE'] and \n",
    "                                                                (x[1].split(',')[1].split('-')[0]=='2014'))\n",
    "for i in filter_ord.take(5): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63013a92-fa31-4ea2-8fc0-199079c1facc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68883\n16831\n"
     ]
    }
   ],
   "source": [
    "print(ord.count())\n",
    "print(filter_ord.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d6309e-cad6-46dc-927c-eb94734144c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##mapValue \n",
    "######take key,value pair as input\n",
    "######do not change the key apply function log to value of the same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b9403f-94f8-4575-a52f-389b0774f728",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('a', (2, 4, 6)), ('b', (6, 8, 10)), ('a', (2, 4, 6, 8, 10))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(((\"a\", (1,2,3)), (\"b\", (3,4,5)),(\"a\", (1,2,3,4,5))))\n",
    "def f(x): return len(x)#length of value \n",
    "def mult(x):return tuple(i*2 for i in x) #mult 2 to each number in vlaue\n",
    "rdd.mapValues(mult).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990bf92c-c814-4510-884f-07b01b6b86da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#join\n",
    "#######join operation will applicable on key value pairs\n",
    "######When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70bb710-9b41-46d7-8e44-affabe3f0782",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ord_key_value\n('1', '11599')\n('2', '256')\n('3', '12111')\n('4', '8827')\n('5', '11318')\norder_items_key_value\n('1', '299.98')\n('2', '199.99')\n('2', '250.0')\n('2', '129.99')\n('4', '49.98')\njoin output\n('4', ('8827', '49.98'))\n('4', ('8827', '299.95'))\n('4', ('8827', '150.0'))\n('4', ('8827', '199.92'))\n('10', ('5648', '199.99'))\nSUBTOTAL\n('4', '49.98299.95150.0199.92')\n('10', '199.9999.96129.9921.99199.99')\n('12', '299.98100.0149.94499.95250.0')\n('16', '119.98299.95')\n('20', '250.0199.92129.99299.95')\n"
     ]
    }
   ],
   "source": [
    "ord_keyvalue=ord.map(lambda x: (x.split(',')[0],x.split(',')[2]))# ord_id,cust_id\n",
    "print('ord_key_value')\n",
    "for i in ord_keyvalue.take(5): print(i)\n",
    "\n",
    "print('order_items_key_value')\n",
    "order_items_keyvalue=order_items.map(lambda x: (x.split(',')[1],x.split(',')[4]))# ord_id,price\n",
    "for i in order_items_keyvalue.take(5): print(i)\n",
    "\n",
    "print('join output')\n",
    "ord_join_order_items=ord_keyvalue.join(order_items_keyvalue)\n",
    "for i in ord_join_order_items.take(5):print(i)\n",
    "print('SUBTOTAL')\n",
    "subtotal=ord_join_order_items.map(lambda x: (x[0],x[1][1])).reduceByKey(lambda x,y:x+y)\n",
    "for i in subtotal.take(5):print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0109f372-ffc3-429e-8c3f-00cc2acef96e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#cogroup\n",
    "######When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable V, Iterable W)) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aeb366e-ed9f-44a7-9cf1-ffdc1266071f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [[1], [2]]\nb [[4], []]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "xy = x.cogroup(y)\n",
    "for i,j in xy.take(2): print(i,list(map(list,j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c1b31ab-eb79-459d-8c60-1a4c814280b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [['49.98', '299.95', '150.0', '199.92'], ['49.98', '299.95', '150.0', '199.92']]\n10 [['199.99', '99.96', '129.99', '21.99', '199.99'], ['199.99', '99.96', '129.99', '21.99', '199.99']]\n12 [['299.98', '100.0', '149.94', '499.95', '250.0'], ['299.98', '100.0', '149.94', '499.95', '250.0']]\n16 [['119.98', '299.95'], ['119.98', '299.95']]\n20 [['250.0', '199.92', '129.99', '299.95'], ['250.0', '199.92', '129.99', '299.95']]\n"
     ]
    }
   ],
   "source": [
    "#list all price corresponding to each order id\n",
    "#creating two rdd from order_items data to apply cogroup\n",
    "order_items.glom().map(len).collect()\n",
    "order_items_1=spark.sparkContext.parallelize(order_items.take(87942))\n",
    "order_items_2=spark.sparkContext.parallelize(order_items.take(84256))\n",
    "\n",
    "#creating keyvalue pairs\n",
    "order_items1_keyvalue=order_items_1.map(lambda x: (x.split(',')[1],x.split(',')[4]))\n",
    "order_items2_keyvalue=order_items_2.map(lambda x: (x.split(',')[1],x.split(',')[4]))\n",
    "#cogroup\n",
    "order_items_cogroup=order_items1_keyvalue.cogroup(order_items2_keyvalue)\n",
    "for i,j in order_items_cogroup.take(5):\n",
    "    print(i,list(map(list,j)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0cd5891-3b27-4563-9b0e-ee3be3411700",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Cartesian(cross join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf93a22-d74a-4d93-b5f6-033e3d8790ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize((1,3,2))\n",
    "sorted(rdd.cartesian(rdd).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca75bc5-5df3-4d01-a6d0-44f7a440d444",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Aggregation operations\n",
    "######Total aggregations – reduce, count (Actions)\n",
    "######By Key aggregations – reduceByKey, aggregrateByKey, groupByKey, countByKey (Transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d20e51e-b4a4-4f1c-8dd3-5f5db9906450",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7556"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Count the number of orders which are closed.\n",
    "ord.filter(lambda x : x.split(',')[3]=='CLOSED').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7070852b-a607-4253-b822-6a1b2c64be61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.0\n62.0\n"
     ]
    }
   ],
   "source": [
    "### Find the total quantity sold for Order ID 1-10.\n",
    "Result=order_items.filter(lambda x : int(x.split(',')[1]) < 11)\\\n",
    "             .map(lambda x : float(x.split(',')[3]))\\\n",
    "             .reduce(lambda x,y : x+y)\n",
    "print(Result)\n",
    "from operator import add\n",
    "Result=order_items.filter(lambda x : int(x.split(',')[1]) < 11).map(lambda x : float(x.split(',')[3])).reduce(add)\n",
    "print(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dccd55f7-eb32-4670-b68a-ee9d21588830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.99\n"
     ]
    }
   ],
   "source": [
    "### For a given order 10 find the maximum subtotal out of all orders.\n",
    "max_subtotal_10=order_items.filter(lambda x : int(x.split(',')[1])==10).map(lambda x : x.split(',')[4]).reduce(lambda a,b : a if \n",
    "(float(a.split(',')[0]) > float(b.split(',')[0])) else b)\n",
    "print(max_subtotal_10)\n",
    "max_subtotal_10=order_items.filter(lambda x : int(x.split(',')[1])==10).map(lambda x : x.split(',')[4]).reduce(max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c088e6-4f08-4507-91c0-bf4bbbc9d063",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Aggregation keys\n",
    "######groupByKey():\n",
    "######aggregrateByKey(): \n",
    "######reduceByKey():\n",
    "######countByKey():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc5f0c1-8082-4aaf-9662-935ad98a97cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#groupByKey(numpartiton=None,partition function)\n",
    "###### When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable V) pairs.\n",
    "######groupByKey perform shuffling .while shuffling create another stage and more i/o operation takes palce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae362e9-7f77-49ed-af31-60cad7a7af2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupByKey output\n957 [299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 2\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n99.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98, 299.98]\nmapValues output\n[('134', 20025.0), ('365', 4421143.020001181), ('502', 3147800.0), ('897', 20566.769999999986), ('957', 4118425.4199997648)]\nmap output\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('957', 4118425.4199997648),\n",
       " ('502', 3147800.0),\n",
       " ('897', 20566.769999999986),\n",
       " ('365', 4421143.020001181),\n",
       " ('134', 20025.0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find aggregated revenue for each product\n",
    "aggregated_revenue = order_items.map(lambda x: (x.split(',')[2], float(x.split(',')[4]))) \\\n",
    "                                .groupByKey() \\\n",
    "                                .mapValues(list)  # Convert values to list directly\n",
    "\n",
    "print('groupByKey output')\n",
    "for i, j in aggregated_revenue.take(1):\n",
    "    print(i, j)  # j is already a list, no need for further conversion\n",
    "\n",
    "print('mapValues output')\n",
    "# Sum the values for each key\n",
    "print(sorted(aggregated_revenue.mapValues(sum).take(5)))\n",
    "\n",
    "print('map output')\n",
    "# Another way to sum the values for each key\n",
    "aggregated_revenue.map(lambda x: (x[0], sum(x[1]))).take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b42b3f3-f4b1-4930-8170-831302b4f08e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#reduceByKey(func,numpartition=none,partitionfunction)\n",
    "###### .When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. \n",
    "###### .reduceByKey() perform combiner which reduce shuffling\n",
    "![shuffling_with_combiner](/dbfs/FileStore/shuffling_with_coimbiner.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e17940d0-ab0b-4981-8772-d0e1114e6c38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('2', [199.99, 250.0, 129.99])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenu_1=order_items.filter(lambda x: int(x.split(',')[1])==2).map(lambda x: (x.split(',')[1], float(x.split(',')[4]))).groupByKey().mapValues(list)\n",
    "revenu_1.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4255e1-4869-4207-8581-36f74b4fb2a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 299.98)\n('4', 699.85)\n('8', 729.8399999999999)\n('9', 599.96)\n('10', 651.9200000000001)\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "#Find total revenue sold for each order.(key=id,value=revenu)\n",
    "total_revenu=order_items.map(lambda x: (x.split(',')[1], float(x.split(',')[4]))).reduceByKey(add)\n",
    "total_revenu=order_items.map(lambda x: (x.split(',')[1], float(x.split(',')[4]))).reduceByKey(lambda x,y:x+y)\n",
    "for i in total_revenu.take(5):print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d6a500-bbb5-42eb-8dac-eecb55d71e64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2', 579.98)\n"
     ]
    }
   ],
   "source": [
    "total_revenu=order_items.map(lambda x: (x.split(',')[1], float(x.split(',')[4]))).reduceByKey(lambda x,y:x+y).filter(lambda x: int(x[0])==2).first()\n",
    "print(total_revenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75697a8b-265a-499b-b6b3-4c4064dcb796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 299.98)\n('4', 299.95)\n('8', 299.95)\n('9', 199.99)\n('10', 199.99)\n"
     ]
    }
   ],
   "source": [
    "#find max revenu for each order. (key=id,value=revenu)\n",
    "max_revenu=order_items.map(lambda x: (x.split(',')[1], float(x.split(',')[4]))).reduceByKey(max)##using max function\n",
    "max_revenu=order_items.map(lambda x: (x.split(',')[1], float(x.split(',')[4]))).reduceByKey(lambda x,y: x if x>y else y )#using max logic\n",
    "for i in max_revenu.take(5):print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62140936-a137-4f8c-a201-38ddfb982d75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, '3,2,502,5,250.0,50.0')\n(4, '6,4,365,5,299.95,59.99')\n(8, '18,8,365,5,299.95,59.99')\n(10, '28,10,1073,1,199.99,199.99')\n(12, '37,12,191,5,499.95,99.99')\n[(12, '37,12,191,5,499.95,99.99'), (10, '28,10,1073,1,199.99,199.99'), (8, '18,8,365,5,299.95,59.99'), (4, '6,4,365,5,299.95,59.99'), (2, '3,2,502,5,250.0,50.0')]\n"
     ]
    }
   ],
   "source": [
    "### Find the maximum revenue for each order. (key=id,value=entire record)\n",
    "total_revenu=order_items.map(lambda x : (int(x.split(',') [1]),x)).reduceByKey(lambda a,b : a if (float(a.split(',')[4]) > float(b.split(',')[4])) else \n",
    "b)\n",
    "for i in total_revenu.take(5):print(i)\n",
    "print(sorted(total_revenu.take(5),reverse=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4964c632-64ef-4e80-87ed-91107c176dcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#aggregateByKey(zeroValue, seqOp, combOp, (numPartitions))\n",
    "###### Zero Value: Initial value to initialize the accumulator. Use 0 for integer and NULL for collections.\n",
    "######SeqOp: Function used to accumulate the results of each partition, and stores the running accumulated result to U. (U,T) => U.\n",
    "######CombOp: Function is used to combine results of all partitions U.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e45dcb4-83c6-4b00-b319-0250df3659d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 250)\n(4, 300)\n(7, 300)\n"
     ]
    }
   ],
   "source": [
    "#Find the maximum revenue for each Order.\n",
    "ordItems=sc.parallelize([\n",
    "(2,\"Joseph\",200), (2,\"Jimmy\",250), (2,\"Tina\",130), (4,\"Jimmy\",50), (4,\"Tina\",300),\n",
    "(4,\"Joseph\",150), (4,\"Ram\",200), (7,\"Tina\",200), (7,\"Joseph\",300), (7,\"Jimmy\",80)],2)\n",
    "#Create a Paired RDD\n",
    "ordPair = ordItems.map(lambda x : (x[0],(x[1],x[2])))\n",
    "#Initialize Accumulator\n",
    "# Zero Value: Zero value in our case will be 0 as we are finding Maximum Marks\n",
    "zero_val=0\n",
    "#Define Sequence Operation\n",
    "# Sequence operation : Finding Maximum revenue from each partition\n",
    "def seq_op(accumulator, element):\n",
    "    if(accumulator > element[1]):\n",
    "        return accumulator \n",
    "    else: \n",
    "        return element[1]\n",
    "#Define Combiner Operation\n",
    "#Combiner Operation : Finding Maximum revenue from all partitions\n",
    "def comb_op(accumulator1, accumulator2):\n",
    "    if(accumulator1 > accumulator2):\n",
    "        return accumulator1\n",
    "    else:\n",
    "        return accumulator2\n",
    "aggr_ordItems = ordPair.aggregateByKey(zero_val, seq_op, comb_op)\n",
    "for i in aggr_ordItems.collect(): print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08de28f-c843-4a23-b536-2c36fc0a5335",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, ('Jimmy', 250))\n(4, ('Tina', 300))\n(7, ('Joseph', 300))\n"
     ]
    }
   ],
   "source": [
    "#Find the maximum revenue for each Order. Print customer name.\n",
    "ordItems=sc.parallelize([\n",
    "(2,\"Joseph\",200), (2,\"Jimmy\",250), (2,\"Tina\",130), (4,\"Jimmy\",50), (4,\"Tina\",300),\n",
    "(4,\"Joseph\",150), (4,\"Ram\",200), (7,\"Tina\",200), (7,\"Joseph\",300), (7,\"Jimmy\",80)],2)\n",
    "#Create a Paired RDD\n",
    "ordPair = ordItems.map(lambda x : (x[0],(x[1],x[2])))\n",
    "#Initialize Accumulator\n",
    "# Zero Value: Zero value in our case will be 0 as we are finding Maximum Marks\n",
    "zero_val=('',0)\n",
    "#Define Sequence Operation\n",
    "# Sequence operation : Finding Maximum revenue from each partition\n",
    "def seq_op(accumulator, element):\n",
    "    if(accumulator[1] > element[1]):\n",
    "        return accumulator \n",
    "    else: \n",
    "        return element\n",
    "#Define Combiner Operation\n",
    "#Combiner Operation : Finding Maximum revenue from all partitions\n",
    "def comb_op(accumulator1, accumulator2):\n",
    "    if(accumulator1[1] > accumulator2[1]):\n",
    "        return accumulator1\n",
    "    else:\n",
    "        return accumulator2\n",
    "aggr_ordItems = ordPair.aggregateByKey(zero_val, seq_op, comb_op)\n",
    "for i in aggr_ordItems.collect(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a806c7-d561-42f3-975e-5d37a749eae4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, (580, 3))\n(4, (700, 4))\n(7, (580, 3))\n"
     ]
    }
   ],
   "source": [
    "#sum up all revenue and number of records for each order.\n",
    "ordItems=sc.parallelize([\n",
    "(2,\"Joseph\",200), (2,\"Jimmy\",250), (2,\"Tina\",130), (4,\"Jimmy\",50), (4,\"Tina\",300),\n",
    "(4,\"Joseph\",150), (4,\"Ram\",200), (7,\"Tina\",200), (7,\"Joseph\",300), (7,\"Jimmy\",80)],2)\n",
    "#Create a Paired RDD\n",
    "ordPair = ordItems.map(lambda x : (x[0],(x[1],x[2])))\n",
    "#Initialize Accumulator\n",
    "# Zero Value: Zero value in our case will be 0 as we are finding Maximum Marks\n",
    "zero_val=(0,0)\n",
    "#Define Sequence Operation\n",
    "# Sequence operation : Sum up all revenue and number of records per partition.\n",
    "def seq_op(accumulator, element):\n",
    "    return (accumulator[0] + element[1], accumulator[1] + 1)\n",
    "#Define Combiner Operation\n",
    "#Combiner Operation : Sum up all revenue and number of records for all partition.\n",
    "def comb_op(accumulator1, accumulator2):\n",
    "    return (accumulator1[0] + accumulator2[0], accumulator1[1] + accumulator2[1])\n",
    "aggr_ordItems = ordPair.aggregateByKey(zero_val, seq_op, comb_op)\n",
    "for i in aggr_ordItems.collect(): print(i)\n",
    "#s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37649554-d2a1-46ec-a5af-9a0e764d6920",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, (250, 3))\n(4, (300, 4))\n(7, (300, 3))\n"
     ]
    }
   ],
   "source": [
    "#max revenue and number of records for each order.\n",
    "ordItems=sc.parallelize([\n",
    "(2,\"Joseph\",200), (2,\"Jimmy\",250), (2,\"Tina\",130), (4,\"Jimmy\",50), (4,\"Tina\",300),\n",
    "(4,\"Joseph\",150), (4,\"Ram\",200), (7,\"Tina\",200), (7,\"Joseph\",300), (7,\"Jimmy\",80)],2)\n",
    "#Create a Paired RDD\n",
    "ordPair = ordItems.map(lambda x : (x[0],(x[1],x[2])))\n",
    "#Initialize Accumulator\n",
    "# Zero Value: Zero value in our case will be 0 as we are finding Maximum Marks\n",
    "zero_val=(0,0)\n",
    "#Define Sequence Operation\n",
    "# Sequence operation : Sum up all revenue and number of records per partition.\n",
    "def seq_op(accumulator, element):\n",
    "    if(accumulator[0]>element[1]):\n",
    "        return (accumulator[0] , accumulator[1] + 1)\n",
    "    else:\n",
    "        return (element[1] , accumulator[1] + 1)\n",
    "#Define Combiner Operation\n",
    "#Combiner Operation : Sum up all revenue and number of records for all partition.\n",
    "def comb_op(accumulator1, accumulator2):\n",
    "    if(accumulator1[0] > accumulator2[0]):\n",
    "        return (accumulator1[0], accumulator1[1] + accumulator2[1])\n",
    "    else:\n",
    "        return (accumulator2[0], accumulator1[1] + accumulator2[1])\n",
    "aggr_ordItems = ordPair.aggregateByKey(zero_val, seq_op, comb_op)\n",
    "for i in aggr_ordItems.collect(): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a2f9fa-41ae-440c-98f7-f717c2a18d19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#countByKey()\n",
    "#######Only available on RDDs of type (K, V). Returns a (K, Int) pairs with the count of each key. \n",
    "######• Returns a Collection Dictionary.FOR OUTPUT WE NEED TO USE items(),keys(),values()\n",
    "######• No shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41aff483-2adb-4b37-a9a7-959e025a1979",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##ITEMS##\n('CLOSED', 7556)\n('PENDING_PAYMENT', 15030)\n('COMPLETE', 22899)\n('PROCESSING', 8275)\n('PAYMENT_REVIEW', 729)\n('PENDING', 7610)\n('ON_HOLD', 3798)\n('CANCELED', 1428)\n('SUSPECTED_FRAUD', 1558)\n##KEYS##\nCLOSED\nPENDING_PAYMENT\nCOMPLETE\nPROCESSING\nPAYMENT_REVIEW\nPENDING\nON_HOLD\nCANCELED\nSUSPECTED_FRAUD\n##VALUES##\n7556\n15030\n22899\n8275\n729\n7610\n3798\n1428\n1558\n"
     ]
    }
   ],
   "source": [
    "ordPair = ord.map(lambda x : (x.split(',')[3],1))\n",
    "countByStatus = ordPair.countByKey()\n",
    "print('##ITEMS##')\n",
    "for i in countByStatus.items() : print(i)\n",
    "print('##KEYS##')\n",
    "for i in countByStatus.keys() : print(i)\n",
    "print('##VALUES##')\n",
    "for i in countByStatus.values() : print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ca583f-c57b-4c07-81ae-e4a35037fd30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###sortByKey(ascending=true,numPartition=None,keyfun=<function<lambda>>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc7f352-9dd9-44bf-8d24-7ea67b61ca0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12435, '41643,2014-04-08 00:00:00.0,12435,PENDING')\n(12435, '61629,2013-12-21 00:00:00.0,12435,CANCELED')\n(12434, '1868,2013-08-03 00:00:00.0,12434,CLOSED')\n(12434, '4799,2013-08-23 00:00:00.0,12434,PENDING_PAYMENT')\n(12434, '5303,2013-08-26 00:00:00.0,12434,PENDING')\n(12434, '6160,2013-09-02 00:00:00.0,12434,COMPLETE')\n(12434, '13544,2013-10-16 00:00:00.0,12434,PENDING')\n(12434, '42915,2014-04-16 00:00:00.0,12434,COMPLETE')\n(12434, '51800,2014-06-14 00:00:00.0,12434,ON_HOLD')\n(12434, '61777,2013-12-26 00:00:00.0,12434,COMPLETE')\n"
     ]
    }
   ],
   "source": [
    "#Sort orders using customer id\n",
    "ordPair = ord.map(lambda x : (int(x.split(',')[2]),x))\n",
    "ordSort = ordPair.sortByKey(ascending=False)\n",
    "for i in ordSort.take(10) : print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4436c5e2-e6ae-43cd-bd1d-c4443162cf07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 'COMPLETE'), '22945,2013-12-13 00:00:00.0,1,COMPLETE')\n((2, 'COMPLETE'), '33865,2014-02-18 00:00:00.0,2,COMPLETE')\n((2, 'COMPLETE'), '67863,2013-11-30 00:00:00.0,2,COMPLETE')\n((2, 'ON_HOLD'), '57963,2013-08-02 00:00:00.0,2,ON_HOLD')\n((2, 'PENDING_PAYMENT'), '15192,2013-10-29 00:00:00.0,2,PENDING_PAYMENT')\n((3, 'COMPLETE'), '22646,2013-12-11 00:00:00.0,3,COMPLETE')\n((3, 'COMPLETE'), '23662,2013-12-19 00:00:00.0,3,COMPLETE')\n((3, 'COMPLETE'), '35158,2014-02-26 00:00:00.0,3,COMPLETE')\n((3, 'COMPLETE'), '57617,2014-07-24 00:00:00.0,3,COMPLETE')\n((3, 'COMPLETE'), '61453,2013-12-14 00:00:00.0,3,COMPLETE')\n"
     ]
    }
   ],
   "source": [
    "#Sort orders using customer and status\n",
    "ordPair = ord.map(lambda x : ((int(x.split(',')[2]), x.split(',')[3]),x))\n",
    "ordSort = ordPair.sortByKey(ascending=True)\n",
    "for i in ordSort.take(10) : print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a0b3f44-6d00-4aff-ae2a-7e499c932fe7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Global Ranking or Ranking per Group\n",
    "######Global Ranking :\n",
    "######• sortByKey and take\n",
    "######• takeOrdered or top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ff1f11-9041-4c5a-bb5f-0e2f8cc0868a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('999.99', '60')\n('999.99', '60')\n('999.99', '60')\n('999.99', '60')\n('999.99', '60')\n('999.99', '60')\n('999.99', '60')\n('999.99', '60')\n('999.99', '60')\n('999.99', '60')\n"
     ]
    }
   ],
   "source": [
    "global_ranking=order_items.map(lambda x:(x.split(',')[4],x.split(',')[2])).sortByKey(ascending=False)\n",
    "for i in global_ranking.take(10):print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e084b6a9-812d-4308-8cfe-3792e73146b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1999.99', '208')\n('1999.99', '208')\n"
     ]
    }
   ],
   "source": [
    "global_ranking=order_items.map(lambda x:(x.split(',')[4],x.split(',')[2])).takeOrdered(2,key= lambda x:-float(x[0]))\n",
    "for i in global_ranking:print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6637bc4-cc60-48d2-a241-40ede77eafbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1999.99', 208)\n('1999.99', 208)\n"
     ]
    }
   ],
   "source": [
    "filter_ranking=order_items.map(lambda x:(x.split(',')[4],int(x.split(',')[2]))).filter(lambda x:x[1]==208)\n",
    "for i in filter_ranking.take(2):print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38670d6a-340f-4265-84b3-fd52a197285c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- `sortByKey`():\n",
    "- This function sorts the entire RDD based on the keys and then returns the sorted RDD.\n",
    "- When you use take(5) on the sorted RDD, it retrieves the first 5 elements from the globally sorted RDD.\n",
    "- `takeOrdered`():\n",
    "- This function retrieves the top N elements directly from the RDD based on the specified key function.\n",
    "- It does not sort the entire RDD but uses a more efficient algorithm to find the top N elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29fc1b2-1b95-4f19-b578-1f3f286d1958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('7650.0', 'product6')\n('600.0', 'product5')\n('550.0', 'product4')\n('440.0', 'product3')\n('230.0', 'product2')\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "order_items_sample = sc.parallelize([\n",
    "    \"1,product1,category1,10.0,3\",\n",
    "    \"2,product2,category2,230.0,1\",\n",
    "    \"3,product3,category3,440.0,2\",\n",
    "    \"4,product4,category4,550.0,2\",\n",
    "    \"5,product5,category5,600.0,2\",\n",
    "    \"6,product6,category6,7650.0,2\"\n",
    "])\n",
    "\n",
    "# Using sortByKey\n",
    "global_ranking_sort = order_items_sample.map(lambda x: (x.split(',')[3], x.split(',')[1])).sortByKey(ascending=False)\n",
    "for i in global_ranking_sort.take(5):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a47bc96-63c6-4ce3-9822-20e1f7bfb83e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('7650.0', 'product6')\n('600.0', 'product5')\n('550.0', 'product4')\n('440.0', 'product3')\n('230.0', 'product2')\n"
     ]
    }
   ],
   "source": [
    "# Using takeOrdered\n",
    "global_ranking_take = order_items_sample.map(lambda x: (x.split(',')[3], x.split(',')[1])).takeOrdered(5, key=lambda x: -float(x[0]))\n",
    "for i in global_ranking_take:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f83e3b0f-693d-4b58-bb3b-d02988dd9431",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Ranking by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536ca0f3-7a7f-41b8-a93b-c244d1525aaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac8eef5-7c15-45eb-a3e6-bd37a6674266",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#union(),intersection(),\n",
    "#subtract(other,no of partition=none),\n",
    "#distinct(no of partition=none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f099d05-920b-4da5-a449-a23a44d9e015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11599\n256\n12111\n8827\n11318\njuly_order_customers:6001\n11607\n5105\n7802\n553\n1604\naugust_order_customers :5680\n"
     ]
    }
   ],
   "source": [
    "july_order=ord.filter(lambda x: x.split(',')[1].split('-')[1]=='07').map(lambda x:x.split(',')[2])\n",
    "for i in july_order.take(5):print (i)\n",
    "print(f'july_order_customers:{july_order.count()}')\n",
    "\n",
    "\n",
    "aug_order=ord.filter(lambda x:x.split(',')[1].split('-')[1]=='08').map(lambda x:x.split(',')[2])\n",
    "for i in aug_order.take(5):print(i)\n",
    "print(f'august_order_customers :{aug_order.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebee32b-267d-45c3-8aa5-4c2f7f1b5997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union output count without distinct:11681\nunion output count without distinct:7633\n"
     ]
    }
   ],
   "source": [
    "july_aug_order=july_order.union(aug_order)#union will take duplicates also\n",
    "print(f\"union output count without distinct:{july_aug_order.count()}\")\n",
    "print(f\"union output count without distinct:{july_aug_order.distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d1afdb-26b3-4dcb-bf51-2372e0934818",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersection output count :1759\n"
     ]
    }
   ],
   "source": [
    "july_aug_order=july_order.intersection(aug_order)#intersection will take duplicates out by applying distinct default\n",
    "print(f\"intersection output count :{july_aug_order.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d59094f-1bd4-44c4-b4b5-93009ded906c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union without distinct:[1, 2, 3, 3, 3, 1, 3, 5]\nunion with distinct:[1, 2, 3, 5]\nintersection:[1, 3]\nsubtract:[2]\nsubtract:[5]\n"
     ]
    }
   ],
   "source": [
    "ex1=sc.parallelize([1,2,3,3,3])\n",
    "ex2=sc.parallelize([1,3,5])\n",
    "print(f\"union without distinct:{ex1.union(ex2).collect()}\")\n",
    "print(f\"union with distinct:{ex1.union(ex2).distinct().collect()}\")\n",
    "print(f\"intersection:{ex1.intersection(ex2).collect()}\")\n",
    "print(f\"subtract:{ex1.subtract(ex2).collect()}\")\n",
    "print(f\"subtract:{ex2.subtract(ex1).collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c950eff2-d1dd-4f2c-a707-59c651213d32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###sample(withReplacement=False,fraction,seed=none)-->transformation\n",
    "####fraction value between 0 to 1\n",
    "####0.1--10%,0.7-->70%\n",
    "####takesample(withReplacement=False,num,seed=none)-->action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766c312d-46da-48a8-8fc2-4f0dbbd227ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n1\n2\n3\n4\n[0, 3, 17, 33, 34]\n[2, 3, 3, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "list1=sc.parallelize(range(100),4)\n",
    "for i in list1.take(5):print(i)\n",
    "output1=list1.sample(withReplacement=False,fraction=0.1,seed=30)\n",
    "print(output1.take(5))\n",
    "output1=list1.sample(withReplacement=True,fraction=0.1,seed=30)#value will repeat. if withReplacement is true\n",
    "print(output1.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbe55b6-3016-40e3-85f0-efd63779d200",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94, 41, 81, 87, 58, 41, 86, 56, 93, 92]\n[92, 72, 24, 65, 42, 69, 53, 97, 18, 68]\n"
     ]
    }
   ],
   "source": [
    "output2=list1.takeSample(withReplacement=True,num=10,seed=30)#duplicate value in output\n",
    "print(output2)\n",
    "output2=list1.takeSample(withReplacement=False,num=10,seed=30)#no duplicate values in output\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d70aca0-776a-4126-80ff-f58f9664cfa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Repartition and coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960a7100-9c42-497c-9d1a-1e4604ff2bbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Create some Dump data for testing\n",
    "df = spark.range(1000000)\n",
    "df = df.select(df.id,df.id*2,df.id*3)\n",
    "df = df.union(df)\n",
    "df = df.union(df)\n",
    "df = df.union(df)\n",
    "df = df.union(df)\n",
    "df = df.union(df)\n",
    "### Convert DataFrame to RDD. \n",
    "RDD = df.rdd.map(lambda x : str(x[0]) + ',' + str(x[1]) + ',' + str(x[2]))\n",
    "### Save the file at a DBFS Path\n",
    "RDD.coalesce(1).saveAsTextFile('/dbfs/user/table/test_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c812dd9-ac4b-46d5-b9c2-79c4cd7a9922",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0,0\n1,2,3\n2,4,6\n3,6,9\n4,8,12\nRDD count:32000000\nRDD no of partaition:3\nrdd_filter count:32\nrdd_filter no of partaition:3\nrdd_filter no of partaition:1\n"
     ]
    }
   ],
   "source": [
    "RDD=sc.textFile('/dbfs/user/table/test_data')\n",
    "for i in RDD.take(5):\n",
    "    print(i)\n",
    "print(f\"RDD count:{RDD.count()}\")   \n",
    "print(f'RDD no of partaition:{RDD.getNumPartitions()}')\n",
    "\n",
    "rdd_filter=RDD.filter(lambda x: x.split(',')[0]=='1')\n",
    "print(f\"rdd_filter count:{rdd_filter.count()}\")\n",
    "print(f'rdd_filter no of partaition:{rdd_filter.getNumPartitions()}')\n",
    "\n",
    "rdd_filter=rdd_filter.coalesce(1)\n",
    "print(f'rdd_filter no of partaition:{rdd_filter.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95751ad-8fac-4dbc-92b6-b946d24b109f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n[34564, 34319]\n5\n[13783, 13770, 13770, 13770, 13790]\n"
     ]
    }
   ],
   "source": [
    "#repartition\n",
    "print(ord.getNumPartitions())\n",
    "print(ord.glom().map(len).collect())\n",
    "re_partiton_ord=ord.repartition(5)\n",
    "print(re_partiton_ord.getNumPartitions())\n",
    "print(re_partiton_ord.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec39779f-756f-4b30-bdb3-2a3ffe2119b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n10\n"
     ]
    }
   ],
   "source": [
    "#coalesce\n",
    "re_partiton_ord.coalesce(10)#coalesce do not increase shuffling but it can decrease shuffling and it will not distribute uniform no of records also\n",
    "print(re_partiton_ord.getNumPartitions())\n",
    "coal_partiton_ord=re_partiton_ord.coalesce(10,shuffle=True)#It will work smilar to repartion\n",
    "print(coal_partiton_ord.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6776345e-2d62-483b-b4cf-f1e5a57a637a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###repartitionAndSortWithinPartitions(no of partition=none,function,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa8b33ec-5b97-401e-ab08-d57e1d991d0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(4, ('a', 'b')), (6, ('j', 'b')), (8, ('s', 'b'))],\n",
       " [(1, ('a', 'b')), (3, ('x', 'f')), (9, ('a', 'z'))]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(((9, ('a','z')), (3, ('x','f')), (6, ('j','b')), (4, ('a','b')), (8, ('s','b')), (1, ('a','b'))),2)\n",
    "rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x : x % 2, True)\n",
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6171f5dd-0ccd-4a82-9297-42fff5b0a260",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###persist() -->used to store intermediate result while processing\n",
    "####storagLevel(disk,memory,offheap,deseralization,replication=1) -->disk,memory\n",
    "##### if we try to change storage level for persist once persist is done it will through a error.So first we need to unprisest and we need to apply new storage level\n",
    "###unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7840199-a595-45e8-98cf-3e7b6176ca7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:444)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1268)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:985)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:946)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:782)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:808)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:807)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:862)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:655)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:444)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1268)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:985)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:946)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:782)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:808)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:807)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:862)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:655)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "df = spark.range(10)\n",
    "print(df.rdd.is_cached)\n",
    "df.rdd.persist()#using persist which take default memory_only\n",
    "print(df.rdd.is_cached)\n",
    "df.rdd.getStorageLevel()#checking type of storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69afe757-877d-4df8-8ded-18ec87e066bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:444)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1268)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:985)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:946)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:782)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:808)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:807)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:862)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:655)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:444)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1268)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:985)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:946)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:782)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:808)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:807)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:862)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:655)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.rdd.unpersist().getStorageLevel()#unpersist and knowning stoagrlevel to apply new storage level\n",
    "df.rdd.persist(StorageLevel.MEMORY_AND_DISK_2). getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32416ad0-c9c2-4391-a4ed-f76bfa40dae8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:444)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1268)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:985)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:946)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:782)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:808)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:807)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:862)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:655)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:444)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1268)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:985)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:946)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:782)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:808)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:807)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:862)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:655)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "person=Row('name','age')\n",
    "person1=person('Alice',10)\n",
    "person1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59eca38b-bde1-41db-8c9b-fd5a2404ba71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##User define functions(UDF)\n",
    "####@udf(returnType=)\n",
    "####udf() function\n",
    "####spark.udf.register()-->sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b59cb3-f821-48d3-97ad-4d7fedea3045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|  emp_name|  new_name|\n+----------+----------+\n|alice jony|Alice Jony|\n| bob smith| Bob Smith|\n+----------+----------+\n\n+----------+----------+\n|  emp_name|  new_name|\n+----------+----------+\n|alice jony|Alice Jony|\n| bob smith| Bob Smith|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "\n",
    "#one way of registering udf using @udf(returnType) -->decorator \n",
    "@udf(returnType=StringType())\n",
    "def initCap(str):\n",
    "    finalStr=\"\"\n",
    "    ar = str.split(\" \")\n",
    "    for word in ar:\n",
    "        finalStr= finalStr + word[0:1].upper() + word[1:len(word)] + \" \" \n",
    "    return finalStr.strip()\n",
    "\n",
    "#DataFrame:\n",
    "emp=Row('emp_id','emp_name')\n",
    "df=spark.createDataFrame([emp(1,'alice jony'),emp(2,'bob smith')],schema='emp_id int,emp_name string')\n",
    "#usinf udf function\n",
    "df.select(df.emp_name, initCap(df.emp_name).alias('new_name')).show()\n",
    "\n",
    "#Spark Sql:\n",
    "#by registering udf for sql use case\n",
    "spark.udf.register(\"initcap1\", initCap)\n",
    "spark.sql('use default')\n",
    "df.createOrReplaceTempView('emp')#create tempview from df\n",
    "spark.sql(\"\"\" select emp_name, initcap1(emp_name) new_name from emp \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994aed1f-0ce0-410a-a6e9-83f219f0628d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# second way of registering upython  function  as udf using udf()\n",
    "import string\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "#python function\n",
    "def initCap(str):\n",
    "    finalStr=\"\"\n",
    "    ar = str.split(\" \")\n",
    "    for word in ar:\n",
    "        finalStr= finalStr + word[0:1].upper() + word[1:len(word)] + \" \" \n",
    "    return finalStr.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40141d7-539c-4d5d-a780-ea68dfb17939",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|  emp_name|  new_name|\n+----------+----------+\n|alice jony|Alice Jony|\n| bob smith| Bob Smith|\n+----------+----------+\n\n+----------+----------+\n|  emp_name|  new_name|\n+----------+----------+\n|alice jony|Alice Jony|\n| bob smith| Bob Smith|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "#registering udf(python_method_name,return_type)\n",
    "initcap_method=udf(initCap,StringType())\n",
    "df.select(df.emp_name, initcap_method(df.emp_name).alias('new_name')).show()\n",
    "\n",
    "#Spark Sql:\n",
    "#by registering udf for sql use case\n",
    "spark.udf.register(\"initcap1\", initCap)\n",
    "spark.sql('use default')\n",
    "df.createOrReplaceTempView('emp')#create tempview from df\n",
    "spark.sql(\"\"\" select emp_name, initcap1(emp_name) new_name from emp \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fec903-086c-4fdf-a4dc-eb6ff7ee49bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|  emp_name|length|\n+----------+------+\n|alice jony|    10|\n| bob smith|     9|\n+----------+------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(slen(test)=4)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using Python Lambda Function and Use it in Spark Sql\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "\n",
    "#dataframe\n",
    "df.select(df.emp_name, slen(df.emp_name).alias('length')).show()\n",
    "\n",
    "#spark sql\n",
    "spark.udf.register(\"slen\", slen)\n",
    "spark.sql(\"SELECT slen('test')\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0352632-6eb1-498d-93d0-47b9eba49f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###types of file reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4cc8f8-62c5-4ff7-a153-8fdacd8cfa8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n\ncsv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n    \n    This function will go through the input once to determine the input schema if\n    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str or list\n        string, or list of strings, for input path(s),\n        or RDD of Strings storing CSV rows.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema\n        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a CSV file and read it back.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a CSV file\n    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n    ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n    ...\n    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n    ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n    +---+----+\n    |age|name|\n    +---+----+\n    |100|NULL|\n    +---+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8265f1c0-4cb0-4850-9290-0fdc8f50c128",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###By using catalog we can  deal with HiveMetastore\n",
    "####Database Functions:\n",
    "- currentDatabase\n",
    "- listDatabases\n",
    "- setCurrentDatabase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a777c5-f7d5-41fd-983b-40409f5563e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\ndefault\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()\n",
    "#creat database\n",
    "spark.sql('create database if not exists test')\n",
    "\n",
    "#Setcurrentdatabase\n",
    "spark.catalog.setCurrentDatabase('test')\n",
    "print(spark.catalog.currentDatabase())\n",
    "spark.catalog.setCurrentDatabase('default')\n",
    "print(spark.catalog.currentDatabase())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74067a1e-f7aa-4e7c-a46e-8e825827e6c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n<class 'pyspark.sql.catalog.Database'>\nDatabase(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='dbfs:/user/hive/warehouse')\ndefault\nTrue\ntest is present \ndefault is present not test \ntest is present \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It will list all database so to find specifi database wether is present are not\n",
    "spark.catalog.listDatabases()\n",
    "print(type(spark.catalog.listDatabases()))\n",
    "print(type(spark.catalog.listDatabases()[0]))\n",
    "print(spark.catalog.listDatabases()[0])\n",
    "print(spark.catalog.listDatabases()[0].name)#default\n",
    "\n",
    "print('test' in [ i .name for i in spark.catalog.listDatabases()])#true \n",
    "\n",
    "[ print(f'{i.name} is present ')  for i in spark.catalog.listDatabases() if i.name=='test']\n",
    "\n",
    "[ print(f'{i.name} is present ') if i.name=='test' else print(f'{i.name} is present not test ')  for i in spark.catalog.listDatabases() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f916d74d-6809-4627-8bff-90c4f32234f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Table Functions:\n",
    "- listColumns\n",
    "- listTables\n",
    "- cacheTable\n",
    "- isCached\n",
    "- uncacheTable\n",
    "- clearCache\n",
    "- recoverPartitions\n",
    "- refreshTable\n",
    "- refreshByPath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b99d76-e2b9-476c-b253-a8387766fd26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method listColumns in module pyspark.sql.catalog:\n\nlistColumns(tableName: str, dbName: Optional[str] = None) -> List[pyspark.sql.catalog.Column] method of pyspark.sql.catalog.Catalog instance\n    Returns a list of columns for the given table/view in the specified database.\n    \n    .. versionadded:: 2.0.0\n    \n    Parameters\n    ----------\n    tableName : str\n        name of the table to list columns.\n    \n        .. versionchanged:: 3.4.0\n           Allow ``tableName`` to be qualified with catalog name when ``dbName`` is None.\n    \n    dbName : str, optional\n        name of the database to find the table to list columns.\n    \n    Returns\n    -------\n    list\n        A list of :class:`Column`.\n    \n    Notes\n    -----\n    The order of arguments here is different from that of its JVM counterpart\n    because Python does not support method overloading.\n    \n    If no database is specified, the current database and catalog\n    are used. This API includes all temporary views.\n    \n    Examples\n    --------\n    >>> _ = spark.sql(\"DROP TABLE IF EXISTS tbl1\")\n    >>> _ = spark.sql(\"CREATE TABLE tblA (name STRING, age INT) USING parquet\")\n    >>> spark.catalog.listColumns(\"tblA\")\n    [Column(name='name', description=None, dataType='string', nullable=True, ...\n    >>> _ = spark.sql(\"DROP TABLE tblA\")\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/catalog.py:680: FutureWarning: `dbName` has been deprecated since Spark 3.4 and might be removed in a future version. Use listColumns(`dbName.tableName`) instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Column(name='ProductID', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='Name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='ProductNumber', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='Color', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='StandardCost', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='ListPrice', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='Size', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='Weight', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='ProductCategoryID', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='ProductModelID', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='SellStartDate', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='SellEndDate', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='DiscontinuedDate', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='ThumbNailPhoto', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='ThumbnailPhotoFileName', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='rowguid', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False),\n",
       " Column(name='ModifiedDate', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(spark.catalog.listColumns)\n",
    "spark.catalog.listColumns('manage_productdetails','default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e663e0a-d2a3-42a0-a909-e53498c31d12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method listTables in module pyspark.sql.catalog:\n\nlistTables(dbName: Optional[str] = None, pattern: Optional[str] = None) -> List[pyspark.sql.catalog.Table] method of pyspark.sql.catalog.Catalog instance\n    Returns a list of tables/views in the specified database.\n    \n    .. versionadded:: 2.0.0\n    \n    Parameters\n    ----------\n    dbName : str, optional\n        name of the database to list the tables.\n    \n        .. versionchanged:: 3.4.0\n           Allow ``dbName`` to be qualified with catalog name.\n    \n    pattern : str, optional\n        The pattern that the database name needs to match.\n    \n        .. versionadded: 3.5.0\n    \n    Returns\n    -------\n    list\n        A list of :class:`Table`.\n    \n    Notes\n    -----\n    If no database is specified, the current database and catalog\n    are used. This API includes all temporary views.\n    \n    Examples\n    --------\n    >>> spark.range(1).createTempView(\"test_view\")\n    >>> spark.catalog.listTables()\n    [Table(name='test_view', catalog=None, namespace=[], description=None, ...\n    \n    >>> spark.catalog.listTables(pattern=\"test*\")\n    [Table(name='test_view', catalog=None, namespace=[], description=None, ...\n    \n    >>> spark.catalog.listTables(pattern=\"table*\")\n    []\n    \n    >>> _ = spark.catalog.dropTempView(\"test_view\")\n    >>> spark.catalog.listTables()\n    []\n\n[Table(name='manage_productdetails', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False), Table(name='manage_productsalesorderdetails', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False), Table(name='manage_salesorderdetails', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]\n[Table(name='emp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "help(spark.catalog.listTables)\n",
    "print(spark.catalog.listTables('default',pattern='manage_*'),end='\\n')\n",
    "print(spark.catalog.listTables('default',pattern='emp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933bb48d-51de-481b-b2cb-444ff6fe1abe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\nTrue\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.uncacheTable('emp')\n",
    "print(spark.catalog.isCached('emp'))\n",
    "spark.catalog.cacheTable('emp')\n",
    "print(spark.catalog.isCached('emp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bf7a50d-d502-4390-9501-98c06bbf9933",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####View Functions:\n",
    "- dropGlobalTempView\n",
    "- dropTempView\n",
    "####Function based functions:\n",
    "- listFunctions\n",
    "- registerFunction (= spark.udf.register)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0f3f96-6792-4610-ac23-42ee3fc0f6ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.dropTempView('emp')\n",
    "print(spark.catalog.listTables('default',pattern='emp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c04672-e4b4-4df3-ac33-b5553505868d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listFunctions()#list all functions you created udf function and recently used inbuilt functions\n",
    "'slen' in [i.name for i in spark.catalog.listFunctions()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a39a3c2-e0dd-47c2-8e78-480947272ac1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##creating new spark session\n",
    "#####every spark has separate SQLConf, registered temporary views and UDFs, but shared SparkContext and table cache(i.e, hivemetastore tables can acess in both sessions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2501ad-d132-4720-b1e3-c9c4d5491d17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|initcap(emp name)|\n+-----------------+\n|         Emp Name|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark\n",
    "import string\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "@udf(returnType=StringType())\n",
    "def initCap(str): \n",
    "    finalStr=\"\"\n",
    "    ar = str.split(\" \") \n",
    "    for word in ar:\n",
    "        finalStr= finalStr + word[0:1].upper() + word[1:len(word)] + \" \"\n",
    "    return string.strip(finalStr)\n",
    "spark.udf.register(\"initcap1\", initCap)\n",
    "spark.sql(\"\"\"select initcap1('emp name') \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6a09ae-fd4d-4744-af1f-67b25c26d040",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4347346809463467>, line 4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m new_spark\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mnewSession()\n",
       "\u001B[1;32m      3\u001B[0m new_spark\n",
       "\u001B[0;32m----> 4\u001B[0m new_spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mselect initcap1(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memp name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m      5\u001B[0m new_spark\u001B[38;5;241m.\u001B[39mstop()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1748\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1744\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1745\u001B[0m         litArgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoArray(\n",
       "\u001B[1;32m   1746\u001B[0m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n",
       "\u001B[1;32m   1747\u001B[0m         )\n",
       "\u001B[0;32m-> 1748\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1749\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1750\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_ROUTINE] Cannot resolve function `initcap1` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`]. SQLSTATE: 42883; line 1 pos 7"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_ROUTINE] Cannot resolve function `initcap1` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`]. SQLSTATE: 42883; line 1 pos 7"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_ROUTINE] Cannot resolve function `initcap1` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`]. SQLSTATE: 42883"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_ROUTINE",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42883",
        "startIndex": 7,
        "stopIndex": 26
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-4347346809463467>, line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m new_spark\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mnewSession()\n\u001B[1;32m      3\u001B[0m new_spark\n\u001B[0;32m----> 4\u001B[0m new_spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mselect initcap1(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memp name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m      5\u001B[0m new_spark\u001B[38;5;241m.\u001B[39mstop()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1748\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1745\u001B[0m         litArgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoArray(\n\u001B[1;32m   1746\u001B[0m             [_to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m [])]\n\u001B[1;32m   1747\u001B[0m         )\n\u001B[0;32m-> 1748\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1749\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1750\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_ROUTINE] Cannot resolve function `initcap1` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`]. SQLSTATE: 42883; line 1 pos 7"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#error because of calling udf defined in another spark session in new sparksession is not possible\n",
    "#The error is occurring because you are trying to call a UDF (initcap1) that was defined in a different Spark session (spark) in a new Spark session (new_spark).\n",
    "new_spark=spark.newSession()\n",
    "new_spark\n",
    "new_spark.sql(\"\"\"select initcap1('emp name') \"\"\").show()\n",
    "new_spark.stop()#it will stop new_spark section"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RDD Notebook 2024-08-19 12:38:45",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
